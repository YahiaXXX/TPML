{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Use the famous Iris dataset, which is available in many machine learning libraries. This dataset consists of 150 samples of iris flowers, each belonging to one of three species: setosa, versicolor, or virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_dataset.csv' with the actual filename/path of your dataset\n",
    "file_path = './dataset.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic statistics about the dataset\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Display information about the dataset, including data types and missing values\n",
    "print(\"\\nDataset Information:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a pair plot to visualize relationships between features and a heatmap of the correlation matrix. Adjust the column names based on your actual dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "subset_df = df.head(100)\n",
    "\n",
    "# Pair plot to visualize relationships between features\n",
    "sns.pairplot(subset_df, hue='Class')  # Assuming 'class' is the column containing the class labels\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix heatmap\n",
    "correlation_matrix = subset_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "    - df_cleaned: DataFrame with missing values handled\n",
    "    \"\"\"\n",
    "\n",
    "    # Display the count of missing values in each column\n",
    "    print(\"Missing Values Before Handling:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Strategy 1: Drop rows with any missing values\n",
    "    df_cleaned = df.dropna()\n",
    "\n",
    "    # Strategy 2: Fill missing numeric values with the mean\n",
    "    # You can customize this strategy based on your data\n",
    "    df_cleaned = df_cleaned.fillna(df_cleaned.mean())\n",
    "\n",
    "    # Display the count of missing values after handling\n",
    "    print(\"\\nMissing Values After Handling:\")\n",
    "    print(df_cleaned.isnull().sum())\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "# Assuming 'your_dataset.csv' is the filename of your dataset\n",
    "file_path = './dataset.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "original_df = pd.read_excel(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "cleaned_df = handle_missing_values(original_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming 'cleaned_df' is your DataFrame\n",
    "# Features (X) and target variable (y)\n",
    "X = cleaned_df.drop(\"Class\", axis=1)\n",
    "y = cleaned_df[\"Class\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "\n",
    "# Use all features for clustering\n",
    "X_clustering = cleaned_df.drop(\"Class\", axis=1)\n",
    "true_labels = cleaned_df['Class']\n",
    "\n",
    "\n",
    "# Implement K-Means clustering\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "kmeans.fit(X_clustering)\n",
    "\n",
    "# Analyze the clusters formed\n",
    "cleaned_df['Cluster'] = kmeans.labels_\n",
    "ari_score = adjusted_rand_score(true_labels, cleaned_df['Cluster'])\n",
    "print(f\"Adjusted Rand Index: {ari_score}\")\n",
    "\n",
    "silhouette_avg = silhouette_score(X, cleaned_df['Cluster'])\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X_clustering['Area'], X_clustering['Perimeter'], c=cleaned_df['Cluster'], cmap='viridis', s=50)\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming cleaned_df is your DataFrame and X_clustering is your features\n",
    "# Change 'Cluster' to the actual column name if it's different\n",
    "attributes = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRation',\n",
    "              'Eccentricity', 'ConvexArea','EquivDiameter', 'Extent', 'Solidity', 'roundness',\n",
    "              'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4']\n",
    "\n",
    "# Create scatter plots for each pair of attributes\n",
    "for i in range(len(attributes)):\n",
    "    for j in range(i + 1, len(attributes)):\n",
    "        attribute1 = attributes[i]\n",
    "        attribute2 = attributes[j]\n",
    "\n",
    "        plt.scatter(X_clustering[attribute1], X_clustering[attribute2], c=cleaned_df['Cluster'], cmap='viridis', s=50)\n",
    "        plt.title(f'K-Means Clustering - {attribute1} vs {attribute2}')\n",
    "        plt.xlabel(attributes[i])\n",
    "        plt.ylabel(attributes[j])\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
